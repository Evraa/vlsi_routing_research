\subsection{Goals}
We want to answer those questions about our proposed algorithm:
\begin{enumerate}
    \item How fast is it?
    \item How short are the found paths?
\end{enumerate}
Because \textit{fast} and \textit{short} are relative terms. We need to calculate them in a comparison with other approaches. 
We chose to compare our results against:
\begin{itemize}
    \item Mikami: known by its speed, but very far from the optimal answer. Would give us insights on how much speed we have achieved.
    \item Steiner Tree: Finds the optimal answer. We will use it to know whether our results are optimal.
    \item Lee Maze: First algorithm used in the industry. Helps us know how far we have progressed relative to the industry practical techniques.
\end{itemize}

\subsection{Code}
We implemented our proposed algorithm in \texttt{mod\_a\_star.py}, 
lee maze algorithm in \texttt{maze\_lee.py},
mikami-tabuchi algorithm in \texttt{mikami\_tabuchi.py}
and Steiner tree in \texttt{steiner\_tree.py}.

All scripts read JSON input from the \texttt{stdin} and writes json output to \texttt{stdout}, so we can chain them with the other scripts. And they follow the \texttt{io\_schema.md} specs about io format.

\subsection{IO Specs}
Input contains a $d\times h \times w$ grid matrix, where:
\begin{itemize}
    \item $d \rightarrow$ Number of layers, either 1 or 2.
    \item $h \rightarrow$ Height.
    \item $w \rightarrow$ Width.
\end{itemize}
Each cell is either:
\begin{itemize}
    \item 0 $\rightarrow$ Empty.
    \item 1 $\rightarrow$ Obstacle.
    \item 2 $\rightarrow$ VIA, only when $d=2$ and must exist on the other layer too.
\end{itemize}
Each input contains the source coordinates and a list of targets coordinates.

Output should contain the found paths and their corresponding lengths.

Both input and output should be in JSON format. See \texttt{io\_schema.md} for more details.

\subsection{Helper Scripts}
We wrote a couple of scripts to assist with the comparison and testing:
\begin{itemize}
    \item \texttt{gen-input.py}: Generates random input that follows \texttt{io\_schema.md}. It doesn't guarantee that all targets are reachable. For more info \texttt{\$ python3 gen-input.py --help}
    \item \texttt{verify.py}: Takes the input to the algorithm and its output and verifies the correctness of the result. See \texttt{\$ python3 verify.py --help}
    \item \texttt{random\_test}: Generates infinite random inputs, run given algorithm on each test, and verify the results.
    \item \texttt{random\_comp}: Generates $N$ random input, run each algorithm on each input, calls \texttt{calc\_total.py} to calculate total cost and verify the results.
    \item \texttt{nConst}: Calls \texttt{random\_comp} $M$ times, each time with same number of targets, varying the grid area (w,h).
    \item \texttt{areaConst}: Calls \texttt{random\_comp} $M$ times, each time with same width and height of the input grid, varying the number of targets.
    \item \texttt{merge\_comp}: Merges the outputs of \texttt{random\_comp} into one \texttt{datasets/summary.json} with the summary of the experiment.
    \item \texttt{plot.py}: Plots given \texttt{summary.json} through the stdin to \texttt{datasets/plot}.
\end{itemize}

\subsection{The Experiment}
For simplicity, we assumed all grids are squares. So $w=h$ in all tests.
We also assumed number of layers $d=2$ in all tests.

We need to very the area while the number of targets is constant. And in the other case, vary the number of targets while the area is constant.
And in both, we will record and plot the running times and costs.

We expect some algorithm to take a very big time to calculate the output. Unfortunately we can't just let it run forever. So we just sat the timeout as 5 minutes. This is why we collected the number of found targets, so we compare how many times an implementation has time-outed and resulted in 0 final targets found.

\textbf{Note:} Not all targets in some input have to be reachable. Bigger grids have bigger probability of having non-reachable targets.

We started the experiments by running:
\begin{enumerate}
    \item \texttt{nConst}, which for each area of areas of grid in [10, 15, 20, 50, 100], conducts 5 random experiments on each algo given the same random input (for each experiment) in which the number of targets ($n$) is const and $n = 5$.
    \item \texttt{areaConst}, which for each number of targets in [6, 10, 15, 20, 50], conducts 20 random experiments on each algo given the same random input (for each experiment) in which the area of the grid ($w,h$) is constant and $w = h = 45$.
\end{enumerate}

After running the 2 scripts multiple times and then merging their outputs using \texttt{merge\_comp}, we had so far 396 unique experiment, each experiment is a unique input given to the 4 algorithms and all the 1584 results are in \texttt{summary.json}.

\subsection{Comparisons}
We made the plots using \texttt{plot.py}. The following is the line of thoughts we had through the comparison.

\subsubsection{Running Time Scatter}
We started by scattering all the running time per \#Targets and per Grid Width, see Fig. (\ref{fig:runningTime}). 

The results are not very clear as the points of mikami and lee are compressed down because of Steiner and a*. 

We can also notice a* has some outliers in time, specially in bigger grid widths (50, 100). This is why we may use the median function multiple times instead of the average.

\begin{figure}
\centering

\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/areaConst.png}
    \caption{Time / \#Targets}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/nConst.png}
    \caption{Time / Grid Width}
\end{subfigure}

\caption{Running Time}
\label{fig:runningTime}
\end{figure}

\subsubsection{Median Running Time}
To make the plots clearer, we calculated the median of the running time instead of scattering all points. See Fig. (\ref{fig:medianRunningTime}).

We notice that both mikami and lee are very fast, they take less than 2 seconds no matter what the input size is.

We also notice that Steiner grows exponentially with both the size of the grid and the number of targets. But it's affected more by the growth of number of targets.

But our new approach grows linearly with only the size of the grid. Its performance is independent from the number of targets.

\begin{figure}
\centering

\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/medianTime_areaConst.png}
    \caption{Median Time / \#Targets}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/medianTime_nConst.png}
    \caption{Median Time / Grid Width}
\end{subfigure}

\caption{Median Running Time}
\label{fig:medianRunningTime}
\end{figure}

\subsubsection{Cost}
Next we move on to comparing cost. Fig. (\ref{fig:maxCost}, \ref{fig:avgCost}, \ref{fig:totalCost}) shows maximum, average and total cost respectively achieved by each algorithm per area and per number of targets.

We notice that a* has achieved overall lower costs in the paths it found, regardless of number of targets or grid area. For some reason it didn't find any on $n=50$. 

All algorithms found no results when grid area = 100. Because \texttt{gen-input.py} generated non-reachable points on that size.

Mikami-Tabuchi was the worst overall in the costs.

\begin{figure}
\centering

\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/maxCost_areaConst.png}
    \caption{Max Path Cost / \#Targets}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/maxCost_nConst.png}
    \caption{Max Path Cost / Grid Width}
\end{subfigure}

\caption{Maximum Path Cost}
\label{fig:maxCost}
\end{figure}

\begin{figure}
\centering

\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/avgCost_areaConst.png}
    \caption{Avg Path Cost / \#Targets}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/avgCost_nConst.png}
    \caption{Avg Path Cost / Grid Width}
\end{subfigure}

\caption{Average Path Cost}
\label{fig:avgCost}
\end{figure}

\begin{figure}
\centering

\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/totalCost_areaConst.png}
    \caption{Total Path Cost / \#Targets}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/totalCost_nConst.png}
    \caption{Total Path Cost / Grid Width}
\end{subfigure}

\caption{Total Path Cost}
\label{fig:totalCost}
\end{figure}

\subsubsection{Errors}
We noticed that some algorithms went to 0 on cost. So we calculated the percentage of found targets by each algorithms, to compare how many targets each algorithms found per $n$. Fig. (\ref{fig:percTargets}) shows that percentage.

The percentage of targets alone don't explain the reason why both a* and Steiner had some zero costs.  Is it because of timeouts? Or because some issues with our code?

Fig. (\ref{fig:timeouts}) is a plot of timeouts per area and per $n$. We notice that both Steiner and a* have high time-outing chances compared to Mikami and lee. Unfortunately, we can't increase the timeout threshold more than 5 minutes, otherwise the nearly 2000 tests won't finish executing.

So far, given this data, we can conclude that the high than average error rate of a* and Steiner are due to their high chances of time-outing.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/plots/percTargets_areaConst.png}
\caption{Percentage of Found Targets / \#Targets}
\label{fig:percTargets}
\end{figure}

\begin{figure}
\centering

\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/timeouts_areaConst.png}
    \caption{\#Timeouts / \#Targets}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{figures/plots/timeouts_nConst.png}
    \caption{\#Timeouts / Grid Width}
\end{subfigure}

\caption{\#Timeouts}
\label{fig:timeouts}
\end{figure}
    